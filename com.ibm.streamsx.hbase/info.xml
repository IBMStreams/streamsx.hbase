<?xml version="1.0" encoding="UTF-8"?>
<!-- Copyright (C) 2013-2018, International Business Machines Corporation -->
<!-- All Rights Reserved                                                   -->
<info:toolkitInfoModel xmlns:common="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:info="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo">
  <info:identity>
    <info:name>com.ibm.streamsx.hbase</info:name>
    <info:description>
The HBase toolkit provides support for interacting with Apache HBase from IBM Streams.

HBase is a Hadoop database, a distributed, scalable, big data store. 
Tables are partitioned by rows across clusters.  
A value in an HBase table is accessed by its row, columnFamily, columnQualifier, and timestamp.
Usually the timestamp is left out, and only the latest value is returned.
The HBase toolkit currently provides no support related to timestamps.

The columnFamily and columnQualifier can collectively be thought of as a column
and are sometimes called that in the APIs.
The separation of the column into two parts allows for some extra flexibility:
the columnFamilies must be defined when the table is established and might be limited, 
but new columnQualifiers can be added at run time and there is no limit to their number.

* Tuples can be added to a HBase table by using the **HBASEPut** operator (which includes a checkAndPut condition) or incremented with the **HBASEIncrement** operator.  
* Tuples can be retrieved with the **HBASEGet** operator from an HBase table.
* The **HBASEScan** operator can output all tuples, or all tuples in a particular row range from an HBase table.
* The **HBASEDelete** operator enables tuples to be deleted from an HBase table.

For some operators, such as **HBASEPut**, the row, columnFamily, columnQualifer, and value must all be specified.
For other operators, such as **HBASEGet** and **HBASEDelete**, the behavior depends on which of those items are specified.
The **HBASEDelete** operator, for example, deletes the whole row if columnFamily and columnQualifier are not specified, 
but it can also be used to delete only a single value.

The columnFamily and columnQualifier (when relevant) can either be specified as an attribute of the input tuple
(columnFamilyAttrName, columnQualifierAttrName), or specified as a single string that is used for all tuples (staticColumnFamily, 
staticColumnQualifier).  The the row and the value (when needed) come from the input tuple.

HBase supports locking by using a check-and-update mechanism for delete and put.
This only locks within a single row, but it allows you to specify either:
 * a full entry (row, columnFamily, columnQualifier, value). If this entry exists with the given value, HBase makes the pure or delete.
 * a partial entry (row, columnFamily, columnQualifier). If there is no value, HBase makes the update.
Note that the row of the put or delete and the row of the check must be the same.
These are scenarios are supported by the **HBASEPut** and **HBASEDelete** operators by specifying a checkAttrName as a parameter.
This attribute on the input stream must be of type tuple and have an attribute of columnFamily and
columnQualifier (with a value if you are doing the first type of check).
In this mode, the operator can have an output port with a success attribute to indicate whether the put or delete happened.

Except for **HBASEIncrement** and **HBASEGet**, the only data types that are currently supported are rstrings.  
**HBASEGet** supports getting a value of type long.   

The com.ibm.streamsx.hbase uses the same configuration information from the **hbase-site.xml** file that HBase does.
For more information about HBase, see [http://hbase.apache.org/].

+ Developing and running applications that use the HBase Toolkit

To create applications that use the HBase Toolkit, you must configure either Streams Studio
or the SPL compiler to be aware of the location of the toolkit. 

# Before you begin

* Install the Apache Ambari on you Hadoop server. https://ambari.apache.org/
  Configure the product environment via ambari and 
  install Apache Hadoop ([http://hadoop.apache.org/])
  Install Apache HBase ([http://hbase.apache.org/]), 
  and Apache Zookeeper ([http://zookeeper.apache.org/]). 
  All of these products can be configured via ambari.

* The com.ibm.streamsx.hbase toolkit can also access to the:
  **IBM Biginsighs** https://www.ibm.com/support/knowledgecenter/en/SSPT3X_4.2.0/com.ibm.swg.im.infosphere.biginsights.welcome.doc/doc/welcome.html
  **Hortonworks** data platform https://hortonworks.com/products/data-platforms/hdp/
 
# About this task

After the location of the toolkit is communicated to the compiler, the SPL artifacts
that are specified in the toolkit can be used by an application.
The application can include a use directive to bring the necessary namespaces into scope.
Alternatively, you can fully qualify the operators that are provided by toolkit with their namespaces as prefixes.

# Procedure

1. For Apache HBase, you must configure your compile and runtime environment by setting the **HBASE_HOME** environment variable:
   * If your Streams application will be running on the same resource where HBase is installed, set **HBASE_HOME** to the directory that contains HBase.
   * Otherwise, copy the **/hbase-install-dir/lib** directory to the file system where Streams will be running, making sure to follow symbolic links:
         cp -Lr path-to-hbase/lib /path-on-streams-host
   * Set **HBASE_HOME** to the directory you created, for example, to **path-on-streams-host**.

2. Supply the operator with HBase configuration information. By default, **HBASE_HOME**/**conf** is searched for the **hbase-site.xml**.  If Streams will not be running on the same resource as the HBase installation, you can do one of the following: 
   * Create a **conf** directory in the same directory you created in step 1 and copy the **hbase-site.xml** from your HBase installation to that directory.
   * Copy the **hbase-site.xml** file from the **conf** directory in your HBase installation into a directory accessible by your application and use the **hbaseSite** parameter to specify the path to the copied file.

3. Configure the SPL compiler to find the toolkit root directory. Use one of the following methods:
   * Set the **STREAMS_SPLPATH** environment variable to the root directory of a toolkit
    or multiple toolkits (with : as a separator).  For example:
       export STREAMS_SPLPATH=$STREAMS_INSTALL/toolkits/com.ibm.streamsx.hbase
   * Specify the **-t** or **--spl-path** command parameter when you run the **sc** command. For example:
       sc -t $STREAMS_INSTALL/toolkits/com.ibm.streamsx.hbase -M MyMain
    where MyMain is the name of the SPL main composite.
    **Note**: These command parameters override the **STREAMS_SPLPATH** environment variable.
   * Add the toolkit location in IBM Streams Studio.

4. Develop your application. To avoid the need to fully qualify the operators, add a use directive in your application. 
   * For example, you can add the following clause in your SPL source file:
       use com.ibm.streamsx.hbase::*;
     You can also specify a use clause for individual operators by replacing the asterisk (\*) with the operator name. For example: 
       use com.ibm.streamsx.hbase::HBASEDelete;

5. Build your application.  You can use the **sc** command or Streams Studio.  

6. Start the IBM Streams instance. Remember to set **HBASE_HOME** and **HADOOP_HOME**
   This can be done using the Streams Console or the **streamtool** utility.
   The streamsx.hbase toolkit from version 3.1.0 delivers all needed apache clint jar libaries. 
   If you use the **hbaseSite** parameter to specify the hbase-site.xml file, it is not necessary to set the **HBASE_HOME** and **HADOOP_HOME** environment variables.    
 

7. Run the application. You can submit the application as a job by using the **streamtool submitjob** command or by using Streams Studio. 

+ What's new
This is an overview of changes for major and minor version upgrades. For details see the Releases in public Github.

++ What is new in version 3.1.0

**Kerberos Authentication**

Kerberos authentication is a network protocol to provide strong authentication for client/server applications.

The streamsx.hbase toolkit support from version 3.1.0 kerberos authentication.
All operators have now 2 additional parameters:

The **authKeytab** parameter specifies the kerberos keytab file that is created for the principal.
The **authPrincipal** parameter specifies the Kerberos principal, which is typically the principal that is created for the HBase server.

If not done already, enable Kerberos authentication on your Hadoop cluster using the following links to find out "how to enable the kerberos authentication".

[https://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s4_kerb_wizard.html]

[https://www.ibm.com/support/knowledgecenter/en/SSPT3X_4.2.0/com.ibm.swg.im.infosphere.biginsights.admin.doc/doc/admin_iop_kerberos.html]



After enabling the Kerberos authentication copy the hbase server keytab and hbase configuration file "hbase-site.xml" from hadoop server into your IBM Streams server in a directory and use them in your SPL application.

Here is an example to connect to the HBase server with Kerberos Authentication:


       () as putsink = HBASEPut(toHBASE)
       {
            param
                authKeytab         : "etc/hbase.service.keytab";
                authPrincipal      : "hbase/myhdp26.ibm.com@HDP2.COM";
                hbaseSite          : "etc/hbase-site.xml";
                rowAttrName        : "key" ;
                tableName          : "streamsSample_books" ; 
                staticColumnFamily : "all" ;
                valueAttrName      : "bookData" ;
       }

++ What is new in version 3.2.0

* The HBASE operators provides a new parameter 'tableNameAttribute'. 
  Use this parameter to pass the table name to the operator via input port. Cannot be used with tableName

* The parameters 'tableNme' and 'tableNameAttribute' are optional, but only one of them must be set to define the name of table.

++ What is new in version 3.3.0

* The HBABSPut parameter 'successAttr' is not depending to the parameter 'checkAttrName'
* It returns in output parameter a boolean results after a successfully insert data into table. 

++ What is new in version 3.4.0
	
* Add a new optional output port for error information. This port submits error message when an error occurs while HBase actions.

++ What is new in version 3.5.0
	
* The pom.xml file has been upgraded to use the Hadoop client version 2.8.5 and the HBase client version 1.4.9.

++ What is new in version 3.6.0
	
* Improvement of HBASEScan operator to support the table name as input port without startRow and endRow.

* Improvement of error output port to support an optional tuple as input tuple.
  If the second attribute of error output port is a tuple like input tuple, 
  the HBASE Operators returns in case of any error, additional to the error message also the input tuple. 
  The first attribute in the optional error output port must be a 'rstring'.
  The second attribute in the error output port is optional and must be a 'TUPLE'.

++ What is new in version 3.7.0

* The HBASEPut operator provides 2 new parameters to add time stamp:  
**Timestamp** This parameter specifies the timestamp in milliseconds (INT64). The timestamp allows for versioning of the cells. Every time HBaes make a PUT on a table it set the timestamp. By default this is the current time in milliseconds, but you can set your own timestamp as well with this parameter. Cannot be used with TimestampAttrName")
**TimestampAttrName** Name of the attribute on the input tuple containing the timestamp in milliseconds. Cannot be used with Timestamp.

The jar library zookeeper-3.4.13.jar has been replaced with **zookeeper-3.4.6.jar** .

++ What is new in version 3.8.0

* The list of 3. party jar libraries updated.
* HBASE version 1.4.10
* HADOOP version 3.1.0
* ZOOKEEPER version 3.4.14

++ What is new in version 3.8.1

* The jar library **netty-all-4.0.52.Final.jar** has been upgraded to **netty-all-4.1.42.Final.jar** .

++ What is new in version 3.8.2

* pom.xml updated to log4j-1.2.17.jar

++ What is new in version 3.8.3

* New error messages for globalization.

++ What is new in version 3.8.4

* pom.xml updated to use commons-configuration2 JAR library version 2.7

++ What is new in version 3.8.5

* pom.xml updated to use commons-codec JAR library version 1.14

++ What is new in version 3.9.0

* The Vulnerability issues for 3rd party libraries have been fixed and the pom.xml updated to use hadoop client version 3.3 JAR libraries.

++ What is new in version 3.9.1

* 3rd party library commons-lang3 added

++ What is new in version 3.9.2

* 3rd party library guava upgraded to version 24.1.1-jre

++ What is new in version 3.9.3

* 3rd party library guava upgraded to version 30.0-jre

++ What is new in version 3.9.4

* 3rd party library slf4j-api upgraded to version 1.7.36
	    
	    
</info:description>
    <info:version>3.9.4</info:version>
    <info:requiredProductVersion>4.0.0.0</info:requiredProductVersion>
  </info:identity>
  <info:dependencies/>
</info:toolkitInfoModel>
